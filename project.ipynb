{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small project\n",
    "\n",
    "We have now seen a large part of Python and some tools that are available, and even if there is much much more to learn we have enough knowledge by now to do a small project together.\n",
    "\n",
    "In this course you have been learning how to use an Arduino to collect sensor data and you have distributed this data with the help of Raspberry Pis. It is now time to look at and analyze the data. We will be using the same tools that we have looked at this week. There will be some minimal data science to just have some new results to look at. Do not let this alarm you, you do not need to understand it, you just need to pay attention to how we visualize it. \n",
    "\n",
    "<img src=\"img/sensors.jpg\" style=\"width:500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have recieved a data set from your project where you have taken readings from three different types of objects and summarized this in a data file located at \"data/iris.data\". Lets start by loading this file using Pandas. The data file in CSV (comma separated values) format which we can easily load through Pandas using `pd.read_csv` that returns a new `DataFrame` with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/iris.data\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `Dataframe.head` we can inspect the first rows of the dataset to get an idea of what is in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataframe.describe` can give us some more detailed information about the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This information can be nice to visualize. `DataFrame`s have the convenient `hist` method that will plot them for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also group the information in a `DataFrame`. In this case it would be convenient to group by class and see a summary of the columns per class. For this we can use the `DataFrame.groupby` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"class\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"class\").describe(percentiles=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"class\").hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also want to visualize the whole dataset using a scatter plot for example.\n",
    "\n",
    "There are four different dimensions (columns) which we would need to reduce the number of dimensions. Here we will use a technique called *PCA* to reduce from four dimensions to three dimensions by calculating the dimensions that have the highest variance, and then we plot the dataset over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reduced = PCA(n_components=3).fit_transform(data.iloc[:, :3])\n",
    "\n",
    "classes = data.replace({\"Iris-setosa\": 0, \"Iris-versicolor\": 1, \"Iris-virginica\": 2}).iloc[:, 4]\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "ax.set_title(\"3D plot of data samples in the three most important dimensions\")\n",
    "\n",
    "p = ax.scatter(data_reduced[:, 0], data_reduced[:, 1], data_reduced[:, 2], c=classes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I would make a guess for the two most significant dimensions I would choose petal width and petal length. Lets make a 2D plot of the samples over these two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data[\"petal-width\"], data[\"petal-length\"], c=classes)\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: predict class of the samples\n",
    "\n",
    "Lets do a small bonus step and try to create a model that can predict the class from values for the sepal and petal dimensions. For this part we will be using [scikit-learn](https://scikit-learn.org/stable/index.html), which is a great machine learning library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step in creating a model is to separate your data in input and output. In our case input is sepal length, sepal height, petal length and petal width. Our output is the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :4] # INPUT: four first columns\n",
    "y = classes          # OUTPUT: the converted version of name to an int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to split our data into something to fit the model to, and a set that we later use to test on. We will make the test set a third of our original data. The `sklearn.model_selection` module has a very handy function `train_test_split` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a model that we want to use to classify the values it recieves. There are many different algorithms for classification problems. Here we have chosen to use *Random Forest*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10) # create a new classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we fit our model to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets then double check how well it performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(Xtest, ytest) # gives us the accuracy of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 98%, not bad!\n",
    "\n",
    "We can also get the importance of each feature from the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can visualize it with the following. \n",
    "\n",
    "(There is some magic here and there, but try and see if you understand it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the feature importances.\n",
    "# np.argsort gives the order of the indices for the values sorted in ascending order.\n",
    "# the last [::-1] reverses the order so we get descending order.\n",
    "indices = np.argsort(rf.feature_importances_)[::-1]\n",
    "\n",
    "# calculate the standard deviation for each importance\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n",
    "             axis=0)\n",
    "\n",
    "# make bar plots of the feature importances and show the standard deviation as well\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Feature importances\")\n",
    "ax.bar(range(4), rf.feature_importances_[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_xticklabels(indices)\n",
    "ax.set_xlim((-1, 4))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the petal length and the petal width are by far the most important features to tell which class a sample belongs to."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
