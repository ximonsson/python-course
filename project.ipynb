{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small project\n",
    "\n",
    "We have now seen a large part of Python and some tools that are available, and even if there is much much more to learn we have enough knowledge by now to do a small project together.\n",
    "\n",
    "In this course you have been learning how to use an Arduino to collect sensor data and you have distributed this data with the help of Raspberry Pis. It is now time to look at and analyze the data. We will be using the same tools that we have looked at this week. There will be some minimal data science to just have some new results to look at. Do not let this alarm you, you do not need to understand it, you just need to pay attention to how we visualize it. \n",
    "\n",
    "<img src=\"img/sensors.jpg\" style=\"width:500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have recieved a data set from your project where you have taken readings from three different types of objects and summarized this in a data file located at \"data/iris.data\". Lets start by loading this file using Pandas. The data file is in CSV (comma separated values) format, which we can easily load through Pandas using `pd.read_csv` that returns a new `DataFrame` with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/ximonsson/python-course/master/data/iris.data\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `Dataframe.head` we can inspect the first rows of the dataset to get an idea of what is in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataframe.describe` can give us some more detailed information about the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This information can be nice to visualize. `DataFrame`s have the convenient `hist` method that will plot them for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also group the information in a `DataFrame`. In this case it would be convenient to group by class and see a summary of the columns per class. For this we can use the `DataFrame.groupby` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"class\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"class\").describe(percentiles=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"class\").hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also want to visualize the whole dataset using a scatter plot for example.\n",
    "\n",
    "There are four different dimensions (columns/features) in the dataset. We would need to reduce the number of dimensions, and we want the three dimensions that will give the best visualization of the dataset. Instead of choosing manually which columns are best to use, we will use a technique called *PCA*. By calculating the dimensions that have the highest variance, the PCA method transforms the four dimensional dataset to a three dimensional one that we can then plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new PCA object that is set to reduce to three dimension.\n",
    "# fit the PCA object and transform the dataset\n",
    "data_reduced = PCA(n_components=3).fit_transform(data.iloc[:, :3])\n",
    "\n",
    "# get an int representation of the classes column.\n",
    "# instead of having a column with the class names as strings, we assign each class an int instead.\n",
    "# then this can be used to pass color information for plotting.\n",
    "# class   class#\n",
    "# A       0\n",
    "# A       0\n",
    "# B       1\n",
    "# B       1\n",
    "# C       1\n",
    "# C       1\n",
    "classes = data.replace({\"Iris-setosa\": 0, \"Iris-versicolor\": 1, \"Iris-virginica\": 2}).iloc[:, 4]\n",
    "\n",
    "# Make a 3D scatter plot\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "ax.set_title(\"3D plot of data samples in the three most important dimensions\")\n",
    "ax.set_xlabel(\"1st most important feature\")\n",
    "ax.set_ylabel(\"2nd most important feature\")\n",
    "ax.set_zlabel(\"3rd most important feature\")\n",
    "ax.scatter(data_reduced[:, 0], data_reduced[:, 1], data_reduced[:, 2], c=classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I would make a guess for the two most significant dimensions I would choose petal width and petal length. Lets make a 2D plot of the samples over these two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data[\"petal-width\"], data[\"petal-length\"], c=classes)\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: predict class of the samples\n",
    "\n",
    "Lets do a small bonus step and try to create a model that can predict the class for a sample of values for the sepal and petal dimensions. For this part we will be using [scikit-learn](https://scikit-learn.org/stable/index.html), which is a great machine learning library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step in creating a model is to separate your data in input and output. In our case input is sepal length, sepal height, petal length and petal width. Our output is the class. We will use the `int` version that we created to give as color information for the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[[\"sepal-length\", \"sepal-width\", \"petal-length\", \"petal-width\"]] # INPUT: four first columns\n",
    "y = classes                                                              # OUTPUT: the converted version of name to an int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to split our data into something to fit the model with, and a set that we later use to test on. We will make the test set a third of our original data. The `sklearn.model_selection` module has a very handy function `train_test_split` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a model that we want to use to classify the values it recieves. There are many different algorithms for classification problems. Here we have chosen to use *Random Forest* using the [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10) # create a new classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we fit our model to the training set using the `RandomForestClassifier.fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets then double check how well it performs on the test set. We can do this by using the `RandomForestClassifier.score` method on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(Xtest, ytest) # gives us the accuracy of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!\n",
    "\n",
    "We can also get the importance of each feature from the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can visualize it with the following. \n",
    "\n",
    "(There is some magic here and there, but try and see if you understand it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the feature importances.\n",
    "# np.argsort gives the order of the indices for the values sorted in ascending order.\n",
    "# the last [::-1] reverses the order so we get descending order.\n",
    "indices = np.argsort(rf.feature_importances_)[::-1]\n",
    "\n",
    "# calculate the standard deviation for each importance\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "\n",
    "# make bar plots of the feature importances and show the standard deviation as well\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Feature importances\")\n",
    "ax.bar(range(4), rf.feature_importances_[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_xticklabels(indices)\n",
    "ax.set_xlim((-1, 4))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the petal length and the petal width are by far the most important features to tell which class a sample belongs to."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
